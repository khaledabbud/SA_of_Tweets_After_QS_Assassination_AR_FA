{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing and stemming: find different words with different roots and count them as one \n",
    "#### For english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "porter = nltk.PorterStemmer()\n",
    "# [porter.stem(t) for t in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization: lighter version of stemming: result are valid words\n",
    "#### For english: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "#[WNlemma.lemmatize(t) for t in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization: split the sentence into words in meaningful way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.word_tokenize(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of speech tagging (POS): determining the role of the word in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing sentence structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank \n",
    "# text = treebank.parsed_sents(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding the frequency of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "# fdist = FreqDist(word for word in tokens)\n",
    "# fdist['wordOfInterest']\n",
    "# fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Recommender\n",
    "\n",
    "Three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender finds the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "Each of the three different recommenders will use a different distance measure (outlined below).\n",
    "\n",
    "Each of the recommenders provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def j_distance01(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    result = dict()\n",
    "    spellings = pd.Series(correct_spellings)\n",
    "    for entry in entries:\n",
    "        ng_entry = set(nltk.ngrams(entry, n=3))\n",
    "        spells = spellings[spellings.str.startswith(entry[0])]\n",
    "        for word in spells:\n",
    "            ng_word = set(nltk.ngrams(word, n=3))\n",
    "            jd_dist = nltk.jaccard_distance(ng_entry, ng_word)\n",
    "            if entry not in result:\n",
    "                result[entry] = (word, jd_dist)\n",
    "            else:\n",
    "                if jd_dist < result[entry][1]:\n",
    "                    result[entry] = (word, jd_dist)\n",
    "    return [result[w][0] for w in result]\n",
    "\n",
    "j_distance01()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cormus', 'incendiary', 'valid']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def j_distance02(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    result = dict()\n",
    "    spellings = pd.Series(correct_spellings)\n",
    "    for entry in entries:\n",
    "        ng_entry = set(nltk.ngrams(entry, n=4))\n",
    "        spells = spellings[spellings.str.startswith(entry[0])]\n",
    "        for word in spells:\n",
    "            ng_word = set(nltk.ngrams(word, n=4))\n",
    "            jd_dist = nltk.jaccard_distance(ng_entry, ng_word)\n",
    "            if entry not in result:\n",
    "                result[entry] = (word, jd_dist)\n",
    "            else:\n",
    "                if jd_dist < result[entry][1]:\n",
    "                    result[entry] = (word, jd_dist)\n",
    "    \n",
    "    \n",
    "    return [result[w][0] for w in result]\n",
    "j_distance02()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def e_distance(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    result = dict()\n",
    "    spellings = pd.Series(correct_spellings)\n",
    "    for entry in entries:\n",
    "        spells = spellings[spellings.str.startswith(entry[0])]\n",
    "        for word in spells:\n",
    "            edit_dist = nltk.edit_distance(entry, word)\n",
    "            if entry not in result:\n",
    "                result[entry] = (word, edit_dist)\n",
    "            else:\n",
    "                if edit_dist < result[entry][1]:\n",
    "                    result[entry] = (word, edit_dist)\n",
    "    \n",
    "    \n",
    "    return [result[w][0] for w in result] \n",
    "e_distance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "clfrNB = naive_bayes.MultinomialNB() # we can change it to alpha=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clfrNB.fit(train_data, train_label)\n",
    "#predicted_labels = clfrNB.predict(test_data)\n",
    "#metrics.f1_score(test_labels, predicted_labels, average = \"micro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM classifier with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clfrSVM = svm.SVC(kernel=\"linear\", C=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clfrSVM.fit(train_data, train_label)\n",
    "#predicted_labels = clfrSVM.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "#X_train, X_test, y_train, y_test = model_selection.train_test_split(train_data, train_labels, test_size = 0.33, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_labels = model_selection.cross_val_predict(clfrSVM, train_data, train_labels, cv = 5) #commonly cv=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "#classifier.classify(unlabled_data)\n",
    "#classifier.classify.many(unlabled_data)\n",
    "#nltk.classify.util.accuracy(classifier, test_srt)\n",
    "#classifier.labels() #gives you all the lables that the classifier has trained on\n",
    "#classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK doesn't have SVM, so we use sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clfrNB = SklearnClassifier(MultinomialNB()).train(train_set)\n",
    "#clfrSVM = SklearnClassifier(SVC(), kernel=\"linear\").train(train_set)\n",
    "# the rest are similar to sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll need to convert the text into a numeric representation that scikit-learn can use.\n",
    "#### The bag-of-words approach is simple and commonly used way to represent text for use in machine learning, which ignores structure and only counts how often each word occurs. CountVectorizer allows us to use the bag-of-words approach by converting a collection of text documents into a matrix of token counts.\n",
    "#### First, we instantiate the CountVectorizer and fit it to our training data.\n",
    "#### Fitting the CountVectorizer consists of the tokenization of the trained data and building of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "# vect = CountVectorizer().fit(X_train)\n",
    "\n",
    "# vect.get_feature_names()[::2000]\n",
    "# len(vect.get_feature_names())\n",
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "# X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This representation is stored in a SciPy sparse matrix, where each row corresponds to a document and each column a word from our training vocabulary.\n",
    "#### We'll use LogisticRegression, which works well for high dimensional sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "# model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "#predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "#print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get the feature names as numpy array\n",
    "#feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "#### Sort the coefficients from the model\n",
    "#sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "#### Find the 10 smallest and 10 largest coefficients\n",
    "#### The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "#### so the list returned is in order of largest to smallest\n",
    "#print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "#print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a different approach, which allows us to rescale features called tf–idf.\n",
    "#### Tf–idf, or Term frequency-inverse document frequency, allows us to weight terms based on how important they are to a document.\n",
    "#### Features with high tf–idf are frequently used within specific documents, but rarely used across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#### Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "#vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "#len(vect.get_feature_names())\n",
    "\n",
    "#X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "#model = LogisticRegression()\n",
    "#model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "#predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "#print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "\n",
    "#feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "#sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "#print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "#print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n",
    "\n",
    "#sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "#print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "#print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizor and tf–idf Vectorizor both take an argument, mindf, which allows us to specify a minimum number of documents in which a token needs to appear to become part of the vocabulary.\n",
    "#### This helps us remove some words that might appear in only a few and are unlikely to be useful predictors. For example, here we'll pass in min_df = 5, which will remove any words from our vocabulary that appear in fewer than five documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fit the CountVectorizer to the training data specifiying a minimum \n",
    "#### document frequency of 5 and extracting 1-grams and 2-grams\n",
    "#vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "#len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression()\n",
    "#model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "#predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "#print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "#sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "#print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "#print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if we want to add more features to the sparse matris to see if we can improve our performance, we can use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for example adding the length of the document as a featute:\n",
    "#X_train_n = add_feature(X_train_vectorized, X_train.apply(len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For text similarity, topic modeling and information extraction go to week for of text mining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
