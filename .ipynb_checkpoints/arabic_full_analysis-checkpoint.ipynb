{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Tweets Sentiment Analysis with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ar', 'en', 'no', '\\tar', 'zr', 'en 2', 'fa'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/tweets_sent_ar.csv\")\n",
    "df['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping English and Persian tweets, leaving only Arabic. I made some mistakes when I labelled the training set which I have to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['language'].isin([\"en\", 'en 2', 'no', 'fa'])].index, inplace=True)\n",
    "df['language'] = 'ar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the unnecessary characters and unifying the different veriety of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(txt):\n",
    "    txt = ' '.join(txt.split())\n",
    "    \n",
    "    replA = (\"أ\", \"إ\", \"آ\")    \n",
    "    chars_to_drop = ( 'ّ', 'َ', 'ِ', 'ُ', 'ْ', 'ً', 'ٍ', 'ٌ', 'ـ' )\n",
    "    chars_to_clean = ('…','#', '@', '-', '_')\n",
    "    txt = txt.replace(\"ة\",\"ه\")\n",
    "    for a in replA:\n",
    "        txt = txt.replace(a, \"ا\")\n",
    "    for d in chars_to_drop:\n",
    "        txt = txt.replace(d,\"\")\n",
    "    for c in chars_to_clean:\n",
    "        txt = txt.replace(c,\" \")\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_unnecessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into labelled and unlabelled sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df[df['sentiment'].isna()]\n",
    "df_t = df.dropna(subset=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(966, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.reset_index(drop = True, inplace = True)\n",
    "df_s.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'O' 'N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laura\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\laura\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.drop(df_t[df_t['sentiment'] == 2].index, inplace=True)\n",
    "df_t.loc[df_t['sentiment'] == 0, 'sentiment'] = 'N'\n",
    "df_t.loc[df_t['sentiment'] == 1, 'sentiment'] = 'O'\n",
    "df_t.loc[df_t['sentiment'] == -1, 'sentiment'] = 'S'\n",
    "df_t.reset_index(drop = True, inplace = True)\n",
    "print(df_t['sentiment'].unique())\n",
    "len(df_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final = df_t.drop(df_t[df_t['sentiment'] == 'N'].index)\n",
    "df_train_final.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the text for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final['tokenized_text'] = df_train_final['cleaned_text'].apply(nltk.word_tokenize)\n",
    "documents = [tuple(x) for x in df_train_final[['tokenized_text', 'sentiment']].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['قال',\n",
       "   'السعوديين',\n",
       "   'عم',\n",
       "   'يتمسخروا',\n",
       "   'انو',\n",
       "   'كيف',\n",
       "   'بيتشيع',\n",
       "   'الشهيد',\n",
       "   'قاسم',\n",
       "   'سليماني',\n",
       "   'بسياره',\n",
       "   'شيفروليه',\n",
       "   'امريكيه',\n",
       "   'الصنع',\n",
       "   'وقال',\n",
       "   'ليش',\n",
       "   'ما',\n",
       "   'بيشيعوه',\n",
       "   'بسياره',\n",
       "   'ايرانيه',\n",
       "   'الصنع',\n",
       "   '.',\n",
       "   'طيب',\n",
       "   'خلص',\n",
       "   'تزعلوش',\n",
       "   'بكرا',\n",
       "   'بس',\n",
       "   'الله',\n",
       "   'ياخد',\n",
       "   'بن',\n",
       "   'سلمان',\n",
       "   'ابقوا',\n",
       "   'شيعوه',\n",
       "   'عالجمل',\n",
       "   'وهيك',\n",
       "   'منكون',\n",
       "   'لاول',\n",
       "   'مره',\n",
       "   'بالتاريخ',\n",
       "   'منشوف',\n",
       "   'فيها',\n",
       "   'جمل',\n",
       "   'حامل',\n",
       "   'بغل'],\n",
       "  'O'),\n",
       " (['الهلال',\n",
       "   'الفيصلي',\n",
       "   'قاسم',\n",
       "   'سليماني',\n",
       "   'هلاك',\n",
       "   'الارهابي',\n",
       "   'قاسم',\n",
       "   'سليماني',\n",
       "   'نطالب',\n",
       "   'بالتشهير',\n",
       "   'للمتحرش',\n",
       "   'ليله',\n",
       "   'عباديات'],\n",
       "  'S'),\n",
       " (['قاسم',\n",
       "   'سليماني',\n",
       "   'اجمل',\n",
       "   'خبر',\n",
       "   'ممكن',\n",
       "   'تسمعه',\n",
       "   'باين',\n",
       "   'سنه',\n",
       "   '2020',\n",
       "   'سنه',\n",
       "   'افراح'],\n",
       "  'S')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(documents)\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for row in df_train_final['tokenized_text']:\n",
    "    all_words += row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in all_words:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "training_set = featuresets[:500]\n",
    "testing_set = featuresets[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB_classifier accuracy percent: 70.62937062937063\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 78.32167832167832\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "#SVC_classifier = SklearnClassifier(SVC())\n",
    "#SVC_classifier.train(training_set)\n",
    "#print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB_classifier accuracy percent: 70.62937062937063\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 83.91608391608392\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 83.21678321678321\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 86.01398601398601\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy percent: 81.11888111888112\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent: 86.01398601398601\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a voting system for the classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy percent: 83.91608391608392\n",
      "Classification: O Confidence %: 85.71428571428571\n",
      "Classification: S Confidence %: 100.0\n",
      "Classification: S Confidence %: 100.0\n",
      "Classification: S Confidence %: 100.0\n",
      "Classification: S Confidence %: 100.0\n",
      "Classification: S Confidence %: 100.0\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    v = voted_classifier.classify(feats)\n",
    "    conf = voted_classifier.confidence(feats)\n",
    "    if conf*100 < 75:\n",
    "        return 'N'\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laura\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_s['sentiment'] = df_s['cleaned_text'].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oppose:  0\n",
      "Support:  114140\n",
      "Neutral:  24\n",
      "====================\n",
      "\n",
      "Without ML\n",
      "Oppose:  21537\n",
      "Suport:  34007\n",
      "Neutral:  58620\n"
     ]
    }
   ],
   "source": [
    "df_op = df_s[df_s['sentiment'] == 'O']\n",
    "print(\"Oppose: \", df_op.shape[0])\n",
    "df_su = df_s[df_s['sentiment'] == 'S']\n",
    "print(\"Support: \", df_su.shape[0])\n",
    "df_neu = df_s[df_s['sentiment'] == 'N']\n",
    "print(\"Neutral: \", df_neu.shape[0])\n",
    "print('====================')\n",
    "print(\"\"\"\n",
    "Without ML\n",
    "Oppose:  21537\n",
    "Suport:  34007\n",
    "Neutral:  58620\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, the accuracy of the model is not impressive at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_arabic = pd.concat([df_s, df_t])\n",
    "df_final_arabic.to_csv(\"Data/df_arabic_classified.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
